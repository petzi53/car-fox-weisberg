{
  "hash": "877b9d64e766b0bffbda95c412980cfb",
  "result": {
    "engine": "knitr",
    "markdown": "# Fitting linear models {#sec-chap04}\n\n\n\n\n\n\n## Table of content for chapter 04 {.unnumbered}\n\n\n::::: {#obj-chap04}\n:::: {.my-objectives}\n::: {.my-objectives-header}\nChapter section list\n:::\n\n::: {.my-objectives-container}\n\n- The linear model (@sec-chap04-1)\n- Linear least-square regression (@sec-chap04-2)\n    - Simple linear regression (@sec-chap04-1)\n\n\n\n:::\n::::\n:::::\n\n## The linear model {#sec-chap04-1}\n\n:::{.my-bulletbox}\n:::: {.my-bulletbox-header}\n::::: {.my-bulletbox-icon}\n:::::\n:::::: {#bul-chap04-lm-assumptions}\n::::::\n: Assumption of the linear model\n::::\n:::: {.my-bulletbox-body}\nThe assumptions of the linear models are:\n\n- **Continuous variable**: A numeric variable that is at least nominally continuous. (Qualitative response variables and count response variables require other regression models. See @sec-chap06.)\n- **Independence**: The observations of the variables for one case are independent of the observations for all other cases. (If cases are dependent, linear mixed-effects models may be more appropriate. See @sec-chap07.)\n- **Linearity**: The dependence of the response on the predictors is through the conditional expected value or *mean function*. (The {**car**} package contains many functions that can help you decide whether the assumption of linearity is reasonable in any given problem. See @sec-chap08.)\n- **Constant conditicional variance**: The conditional variance of the response given the regressors (or, equivalently, the predictors) is constant. (See @sec-chap05-1 --- @sec-chap05-3 and @sec-chap08-5 for diagnostics tools.)\n\n***\n\nChanging assumptions changes the model. For example, it is common to add a normality assumption, producing the *normal linear model*. Another common extension to the linear model is to modify the constant variance assumption producing the *weighted linear model*. \n\n::::\n:::\n\n\n## Linear least-square regression {#sec-chap04-2}\n\n### Simple linear regressio9n {#sec-chap04-2-1}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-skim-Davis-data}\n: A quick look at the Davis data\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap04-skim-Davis-data}\n\n::: {.cell}\n\n```{.r .cell-code}\nDavis <- carData::Davis\n\nskimr::skim(Davis)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |      |\n|:------------------------|:-----|\n|Name                     |Davis |\n|Number of rows           |200   |\n|Number of columns        |5     |\n|_______________________  |      |\n|Column type frequency:   |      |\n|factor                   |1     |\n|numeric                  |4     |\n|________________________ |      |\n|Group variables          |None  |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts    |\n|:-------------|---------:|-------------:|:-------|--------:|:-------------|\n|sex           |         0|             1|FALSE   |        2|F: 112, M: 88 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|    sd|  p0|   p25|   p50|    p75| p100|hist  |\n|:-------------|---------:|-------------:|------:|-----:|---:|-----:|-----:|------:|----:|:-----|\n|weight        |         0|          1.00|  65.80| 15.10|  39|  55.0|  63.0|  74.00|  166|▇▆▁▁▁ |\n|height        |         0|          1.00| 170.02| 12.01|  57| 164.0| 169.5| 177.25|  197|▁▁▁▇▇ |\n|repwt         |        17|          0.92|  65.62| 13.78|  41|  55.0|  63.0|  73.50|  124|▆▇▃▁▁ |\n|repht         |        17|          0.92| 168.50|  9.47| 148| 160.5| 168.0| 175.00|  200|▃▇▇▃▁ |\n\n\n:::\n:::\n\n\nSkim the Davis data set\n:::\n\n::::\n:::::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-lm-davis}\n: Simple linear regression \n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### davis_lm0\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-print-davis-lm0}\n: Simple linear regression of Davis data\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap04-print-davis-lm0}\n\n::: {.cell}\n\n```{.r .cell-code}\n(davis_lm0 <- lm(weight ~ repwt, data = Davis))\n\nglue::glue(\"\")\nglue::glue(\"Suppressing the intercept to force the regression through the origin\")\nlm(weight ~ -1 + repwt, data = Davis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = weight ~ repwt, data = Davis)\n#> \n#> Coefficients:\n#> (Intercept)        repwt  \n#>      5.3363       0.9278  \n#> \n#> \n#> Suppressing the intercept to force the regression through the origin\n#> \n#> Call:\n#> lm(formula = weight ~ -1 + repwt, data = Davis)\n#> \n#> Coefficients:\n#> repwt  \n#> 1.006\n```\n\n\n:::\n:::\n\nSimple\tlinear\tregression\tmodel of the Davis data\n:::\n\n***\n\nThe `stats::lm()` function returns a linear-model object, which was assigned to the R variable `davis_lm0`. Printing this object just results in a relatively sparse output: Printing a linear-model object simply shows the command that produced the model along with the estimated regression coefficients.\n\nWe can suppress the intercept to force the regression through the origin using `weight ~ -1 + repwt` or `weight ~ repwt - 1`. More generally, a minus sign removes a term, here the intercept, from the predictor. Using $0$ (zero) also suppresses the intercept. That $0$ is an equivalent of $-1$ can be seen in the repetition of the formula which uses $0$ although I have used in the formula $-1$.\n\n::: {.callout-caution style=\"color: darkgoldenrod;\" #cau-chap04-without-intercept}\n##### Difference in coefficient without intercept\n\nThere is a small difference in the coefficient with and without intercept. Where does this come from and what does it mean?\n\nIs this the result \"to force the regression through the origin\"?\n:::\n\n\n::::\n:::::\n\n\n\n###### stats::summary()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-summary-davis-lm0}\n: Summary of the linear model `davis_lm0`\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-listing-ID}    \n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(davis_lm0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = weight ~ repwt, data = Davis)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#>  -7.048  -1.868  -0.728   0.601 108.705 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)   5.3363     3.0369   1.757   0.0806\n#> repwt         0.9278     0.0453  20.484   <2e-16\n#> \n#> Residual standard error: 8.419 on 181 degrees of freedom\n#>   (17 observations deleted due to missingness)\n#> Multiple R-squared:  0.6986,\tAdjusted R-squared:  0.697 \n#> F-statistic: 419.6 on 1 and 181 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nUsing stats::summary() for the `davis_lm0` object\n:::\n\n::::\n:::::\n\n###### car::S()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-car-S-davis-lm0}\n: Default usage of the more sophisticated summary with `car::S()`\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap04-car-S-davis-lm0}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::S(davis_lm0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Call: lm(formula = weight ~ repwt, data = Davis)\n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)   5.3363     3.0369   1.757   0.0806\n#> repwt         0.9278     0.0453  20.484   <2e-16\n#> \n#> Residual standard deviation: 8.419 on 181 degrees of freedom\n#>   (17 observations deleted due to missingness)\n#> Multiple R-squared: 0.6986\n#> F-statistic: 419.6 on 1 and 181 DF,  p-value: < 2.2e-16 \n#>     AIC     BIC \n#> 1303.06 1312.69\n```\n\n\n:::\n:::\n\n\nDefault summary output of `car::S()`\n:::\n\n***\n\nAs a subset of the stats:summary() printout it does not report the residual distribution but adds  with AIC and BIC two diagnostics figures for compare and choose between different models.\n\n- **Call**: A brief header is printed first, repeating the command that created the regression model. \n- **Coefficients**: The part of the output marked `Coefficients` provides basic information about the estimated regression coefficients. \n    - The **row names** (=first column) lists the names of the regressors fit by `lm()`. \n        - **Intercept**: The intercept, if present, is named `(Intercept)`. \n        - **Estimate**: The column labeled `Estimate` provides the least-squares estimates of the regression coefficients. \n        - **Std. Error**: The column marked `Std. Error` displays the standard errors of the estimated coefficients. The default standard errors are computed as $\\sigma$ times a function of the regressors, as presented in any regression text, but the `car::S()` function also allows you to use other methods for computing standard errors. \n        - **t-value**: The column marked `t value` is the ratio of each estimate to its standard error and is a <a class='glossary' title='Wald test is the statistical test for comparing the value of the coefficient in linear or logistic regression to the hypothesized value of zero; the form is similar to a one-sample t-test, although some Wald tests use a t-statistic and others use a z-statistic as the test statistic. (SwR, Glossary)'>Wald test statistic</a> for the null hypothesis that the corresponding population regression coefficient is equal to zero. If assumptions hold and the errors are normally distributed or the sample size is large enough, then these <a class='glossary' title='The T-Statistic is used in a T test when you are deciding if you should support or reject the null hypothesis. It’s very similar to a Z-score and you use it in the same way: find a cut off point, find your t score, and compare the two. You use the t statistic when you have a small sample size, or if you don’t know the population standard deviation. (Statistics How-To)'>t-statistics</a> computed with the default <a class='glossary' title='The standard error (SE) of a statistic is the standard deviation of its [sampling distribution]. If the statistic is the sample mean, it is called the standard error of the mean (SEM). (Wikipedia) The standard error is a measure of variability that estimates how much variability there is in a population based on the variability in the sample and the size of the sample. (SwR, Glossary)'>standard error</a> estimates are distributed under the null hypothesis as t random variables with <a class='glossary' title='Degree of Freedom (df) is the number of pieces of information that are allowed to vary in computing a statistic before the remaining pieces of information are known; degrees of freedom are often used as parameters for distributions (e.g., chi-squared, F). (SwR, Glossary)'>degrees of freedom</a> (df ) equal to the <a class='glossary' title='Residuals are the differences between the observed values and the predicted values. (SwR, Glossary)'>residual</a> degrees of freedom under the model. \n        - **Pr (>|t|)**: The column marked `Pr (>|t|)` is the two-sided <a class='glossary' title='The p-value is the probability that the test statistic is at least as big as it is under the null hypothesis (SwR, Glossary)'>p-value</a> for the null hypothesis assuming that $\\beta_{0} = 0$\tversus\t$\\beta_{0} \\neq 0$ the t-distribution is appropriate. (*Example*: The hypothesis $\\beta_{0} = 0$\tversus\t$\\beta_{0} != 0$, with the value of $\\beta_{1}$ unspecified, has a p-value of about .08, providing weak evidence against the null hypothesis that $\\beta_{0} = 0$, if the assumptions of the model hold.)\n    - Below the coefficient table is additional summary information, including the **residual standard deviation**. (*Example*: This typical error in prediction is so large that, if correct, it is unlikely that the predictions of actual weight from reported weight would be of any practical value.)\n    - The **residual df** are n − 2 for simple regression, here 183 − 2 = 181, and we’re alerted to the fact that 17 cases are removed because of missing data. \n    - The **Multiple R-squared**, $R^2 \\approx 0.70$, is the square of the correlation between the response and the fitted values and is interpretable as the proportion of variation of the response variable around its mean accounted for by the regression. \n    - **F-statistic**: The reported <a class='glossary' title='F-statistic is a test statistic comparing explained and unexplained variance in [ANOVA] and linear regression. The F-statistic is a ratio where the variation between the groups is compared to the variation within the groups. (SwR, Glossary)'>F-statistic</a> provides a <a class='glossary' title='The likelihood ratio test is a test that compares two nested binary logistic regression models to determine which is a better fit to the data; the difference between two log-likelihoods follows a chi-squared distribution with a significant result indicating the larger model is a better fitting model. (SwR, Glossary)'>likelihood-ratio</a> test of the general null hypothesis that all the population regression coefficients, except for the intercept, are equal to zero, versus the alternative that at least one of the $\\beta_{j}$ is nonzero. If the errors are normal or n is large enough, then this test statistic has an F-distribution with the degrees of freedom shown. Because simple regression has only one parameter beyond the intercept, the F-test is equivalent to the t-test that $\\beta_{1} = 0$, with $t_{2} = F$. (In other models, such as GLMs or normal nonlinear models, Wald tests and the generalization of the likelihood-ratio F-test may test the same hypotheses, but they need not provide the same inferences.) \n    - The **AIC** and **BIC** values at the bottom of the output are, respectively, the Akaike Information Criterion and the Bayesian Information Criterion, statistics that are sometimes used for model selection.\n\n::::\n:::::\n\n\n\n\n###### car::brief()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-car-brief-davis-lm0}\n: Short summary of the lm object with `car::brief()`\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap04-car-brief-davis-lm0}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::brief(davis_lm0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            (Intercept)  repwt\n#> Estimate          5.34 0.9278\n#> Std. Error        3.04 0.0453\n#> \n#>  Residual SD = 8.42 on 181 df, R-squared = 0.699\n```\n\n\n:::\n:::\n\n\nBrief summary of the lm object with `car::brief()`\n:::\n\n::::\n:::::\n\n###### stats::confint()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-stats-confint-davis-lm0}\n: Confidence intervals for the estimates of `davis_lm0`  with `stats::confint()`\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap04-stats-confint-davis-lm0}\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::confint(davis_lm0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                  2.5 %    97.5 %\n#> (Intercept) -0.6560394 11.328560\n#> repwt        0.8384665  1.017219\n```\n\n\n:::\n:::\n\n\nConfidence intervals for the estimates of `davis_lm0` with `stats::confint()`\n:::\n\n::::\n:::::\n\n###### car::Confint()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-car-Confint-davis-lm0}\n: Confidence intervals for the estimates of `davis_lm0` with `car::Confint()`\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap04-car-Confint-davis-lm0}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::Confint(davis_lm0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>              Estimate      2.5 %    97.5 %\n#> (Intercept) 5.3362605 -0.6560394 11.328560\n#> repwt       0.9278428  0.8384665  1.017219\n```\n\n\n:::\n:::\n\n\nConfidence intervals for the estimates of `davis_lm0` with `car::Confint()`\n:::\n\n***\n\nUnlike `stats::confint()`, `car::Confint()` prints the coefficient estimates along with the confidence limits.\n\n::::\n:::::\n\n\n\n:::\n\n::::\n:::::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}