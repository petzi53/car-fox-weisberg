{
  "hash": "9ab5ce5d66ec7f2049ecf59b6f65281a",
  "result": {
    "engine": "knitr",
    "markdown": "# Getting started with R & RStudio {#sec-chap01}\n\n\n\n\n\n\n## Table of content for chapter 01\n\n::::: {#obj-chap01}\n:::: {.my-objectives}\n::: {.my-objectives-header}\nChapter section list\n:::\n\n::: {.my-objectives-container}\n\n-   ~~Working with RStudio projects (empty)~~ (@sec-chap01-1)\n-   R Basics (@sec-chap01-2)\n-   Fixing errors and getting help (@sec-chap01-3)\n-   ~~Organizing your work and making it reproducible (empty)~~ (@sec-chap01-4)\n-   An Extended Illustration: Duncan's Occupational-Prestige Regression (@sec-chap01-5)\n    - Getting, recoding and showing data (@sec-chap01-5-1)\n    - Explorative data analysis (@sec-chap01-5-2)\n    - Regression analysis (@sec-chap01-5-3)\n    - Regression diagnostics (@sec-chap01-5-4)\n- ~~R Functions for Basic Statistics (empty)~~ (@sec-chap01-6)\n- Generic functions and their methods (@sec-chap01-7)\n\n\n\n:::\n::::\n:::::\n\n## Working with RStudio projects (empty) {#sec-chap01-1}\n\n## R Basics {#sec-chap01-2}\n\n- A novel feature of the R help system is the facility it provides to execute most examples in the help pages via the example() command: \n$$example ('log')$$\n\n- A quick way to determine the arguments of an R function is to use the `args()` function:\n$$args ('log')$$\n\n::: {.callout-warning #wrn-help-generig-functions}\nThe `args()` and `help()` functions may not be very helpful with generic functions.\n:::\n\n- For the full set of reserved symbols in R, see \n$$help ('Reserved')$$\n\n- There are two types of logical operators: \n    - vectorizes: $\\&$ and $|$ vectorized resp. \n    - single operand: $\\&\\&$ and $||$\n    \nThe unvectorized versions of the *and* (&&) and *or* (||) operators are primarily useful for writing R programs and are not appropriate for indexing vectors.\n\n## Fixing errors and getting help {#sec-chap01-3}\n\n:::{.my-bulletbox}\n:::: {.my-bulletbox-header}\n::::: {.my-bulletbox-icon}\n:::::\n:::::: {#bul-getting-help}\n::::::\n: Getting help\n::::\n:::: {.my-bulletbox-body}\n- `base::traceback()` provides information about the sequence of function calls leading up to an error.\n- `utils::apropos('<searchString>')` searches for currently accessible objects whose names contain a particular character string. It returns a character vector giving the names of objects in the search list matching (as a regular expression)\n- `utils:find('<searchString>')` returns where objects of a given name can be found.\n- `??` or `help.search()` activates a broader search because it looks not only in the title fields but in other fields of the help pages as well.\n- `utils::RSiteSearch()` or the search engine at https://search.r-project.org/ needs an internet connection and starts a search even broader. It looks in all standard and CRAN packages, even those not installed on your system.\n- [CRAN task views](https://cran.r-project.org/web/views/) describe\tresources in R for applications in specific areas.\n    - Views can be installed automatically via ctv::install.views(\"Bayesian\") or ctv::update.views(\"Bayesian\", coreOnly=TRUE)\n    - Query information about a particular task view on CRAN from within R for example with `ctv::ctv(\"MachineLearning\")`\n    - Query to obtain the list of all task views available with `ctv::available.views()`\n- `help(package='<packageName>')` calls the index help page of an installed package in the RStudio Help tab, including the hyperlinked index of help topics documented in the package.\n- <a class='glossary' title='A vignette is a long-form guide to your package. (Chapter Vignettes in R Packages 2e)'>Vignette</a>:\n    - `utils::vignette()` lists available vignettes in the packages installed on your system in the code window. \n    - `utils::browseVignettes()` open a local web page listing vignettes in the packages installed on your system\n    - `utils::vignette(package='<packageName>')` displays the vignettes available in a particular installed package. \n    - `vignette('vignetteName')` or vignette('<vignetteName>', package='<packageName>') opens a specific vignette.\n- *RStudio help*: \n    - Menu \"Help > R Help\" opens  an overview page with R resources\n    - Search R help with shortcut CTRL-ALT-F1\n    - [Finding your way to R](https://education.rstudio.com/learn/) with three learning pathes: [Beginners](https://education.rstudio.com/learn/beginner/), [Intermediates](https://education.rstudio.com/learn/intermediate/) and [Experts](https://education.rstudio.com/learn/expert/)\n- *Google search*\n- <a class='glossary' title='Stack Overflow is a question-and-answer website for computer programmers. (Wikipedia)'>StackOverflow</a> and <a class='glossary' title='Cross Validated is a question and answer site for people interested in statistics, machine learning, data analysis, data mining, and data visualization. It is built and run by you as part of the Stack Exchange network of Q&amp;A sites. (StackExchange)'>Cross Validated</a>: \n    - [Stack Overflow](https://stackoverflow.com/) is a question and answer site for professional and enthusiast programmers. \n    - [Cross Validated](https://stats.stackexchange.com/) is a question and answer site for people interested in statistics, machine learning, data analysis, data mining, and data visualization.\n\n::::\n:::\n\n\n\n## Organizing your work and making it reproducible (empty) {#sec-chap01-4}\n\n## An Extended Illustration {#sec-chap01-5}\n\n::: {.callout-warning #wrn-chap01-illustration-not-understanding}\n##### Only illustration --- not yet understanding\n\nI will follow all steps of the illustration with the Duncan data set. But be aware that I am still lacking understanding of all the procedures. Hopefully these gaps will filled with the next chapters. \n\nI am planning whenever I find an explication of the routines that follow I will provide a link to cross reference illustration and explanation.\n:::\n\n\n### Getting, recoding and showing data {#sec-chap01-5-1}\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap01-illustration}\n: Duncan's Occupational-Prestige Regression\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### get data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-show-duncan-data}\n: Show Duncan raw data from {**carData**} package\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-get-duncan-data}\n\n::: {.cell}\n\n```{.r .cell-code}\ncarData::Duncan\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                    type income education prestige\n#> accountant         prof     62        86       82\n#> pilot              prof     72        76       83\n#> architect          prof     75        92       90\n#> author             prof     55        90       76\n#> chemist            prof     64        86       90\n#> minister           prof     21        84       87\n#> professor          prof     64        93       93\n#> dentist            prof     80       100       90\n#> reporter             wc     67        87       52\n#> engineer           prof     72        86       88\n#> undertaker         prof     42        74       57\n#> lawyer             prof     76        98       89\n#> physician          prof     76        97       97\n#> welfare.worker     prof     41        84       59\n#> teacher            prof     48        91       73\n#> conductor            wc     76        34       38\n#> contractor         prof     53        45       76\n#> factory.owner      prof     60        56       81\n#> store.manager      prof     42        44       45\n#> banker             prof     78        82       92\n#> bookkeeper           wc     29        72       39\n#> mail.carrier         wc     48        55       34\n#> insurance.agent      wc     55        71       41\n#> store.clerk          wc     29        50       16\n#> carpenter            bc     21        23       33\n#> electrician          bc     47        39       53\n#> RR.engineer          bc     81        28       67\n#> machinist            bc     36        32       57\n#> auto.repairman       bc     22        22       26\n#> plumber              bc     44        25       29\n#> gas.stn.attendant    bc     15        29       10\n#> coal.miner           bc      7         7       15\n#> streetcar.motorman   bc     42        26       19\n#> taxi.driver          bc      9        19       10\n#> truck.driver         bc     21        15       13\n#> machine.operator     bc     21        20       24\n#> barber               bc     16        26       20\n#> bartender            bc     16        28        7\n#> shoe.shiner          bc      9        17        3\n#> cook                 bc     14        22       16\n#> soda.clerk           bc     12        30        6\n#> watchman             bc     17        25       11\n#> janitor              bc      7        20        8\n#> policeman            bc     34        47       41\n#> waiter               bc      8        32       10\n```\n\n\n:::\n:::\n\n\nDuncan raw data from {**carData**} package\n\n:::\n\n***\nThe row names contains data and have to be therefore a separate column.\n\n\n::::\n:::::\n\n\n###### recode \n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-recode-duncan-data}\n: Recode Duncan data\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-recode-duncan-data}    \n\n::: {.cell}\n\n```{.r .cell-code}\n(d01.1 <- carData::Duncan |> \n    tibble::rownames_to_column(\"occupation\"))\n\nsave_data_file(\"chap01\", d01.1, \"d01.1.rds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            occupation type income education prestige\n#> 1          accountant prof     62        86       82\n#> 2               pilot prof     72        76       83\n#> 3           architect prof     75        92       90\n#> 4              author prof     55        90       76\n#> 5             chemist prof     64        86       90\n#> 6            minister prof     21        84       87\n#> 7           professor prof     64        93       93\n#> 8             dentist prof     80       100       90\n#> 9            reporter   wc     67        87       52\n#> 10           engineer prof     72        86       88\n#> 11         undertaker prof     42        74       57\n#> 12             lawyer prof     76        98       89\n#> 13          physician prof     76        97       97\n#> 14     welfare.worker prof     41        84       59\n#> 15            teacher prof     48        91       73\n#> 16          conductor   wc     76        34       38\n#> 17         contractor prof     53        45       76\n#> 18      factory.owner prof     60        56       81\n#> 19      store.manager prof     42        44       45\n#> 20             banker prof     78        82       92\n#> 21         bookkeeper   wc     29        72       39\n#> 22       mail.carrier   wc     48        55       34\n#> 23    insurance.agent   wc     55        71       41\n#> 24        store.clerk   wc     29        50       16\n#> 25          carpenter   bc     21        23       33\n#> 26        electrician   bc     47        39       53\n#> 27        RR.engineer   bc     81        28       67\n#> 28          machinist   bc     36        32       57\n#> 29     auto.repairman   bc     22        22       26\n#> 30            plumber   bc     44        25       29\n#> 31  gas.stn.attendant   bc     15        29       10\n#> 32         coal.miner   bc      7         7       15\n#> 33 streetcar.motorman   bc     42        26       19\n#> 34        taxi.driver   bc      9        19       10\n#> 35       truck.driver   bc     21        15       13\n#> 36   machine.operator   bc     21        20       24\n#> 37             barber   bc     16        26       20\n#> 38          bartender   bc     16        28        7\n#> 39        shoe.shiner   bc      9        17        3\n#> 40               cook   bc     14        22       16\n#> 41         soda.clerk   bc     12        30        6\n#> 42           watchman   bc     17        25       11\n#> 43            janitor   bc      7        20        8\n#> 44          policeman   bc     34        47       41\n#> 45             waiter   bc      8        32       10\n```\n\n\n:::\n:::\n\n\nDuncan data recoded\n:::\n\n***\n\nDuncan was interested in how `prestige` is related to `income` and `education` in combination.\n\n::::\n:::::\n\n###### skim data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-skim-duncan}\n: Skim Duncan data\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-skim-duncan}\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(d01.1)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |      |\n|:------------------------|:-----|\n|Name                     |d01.1 |\n|Number of rows           |45    |\n|Number of columns        |5     |\n|_______________________  |      |\n|Column type frequency:   |      |\n|character                |1     |\n|factor                   |1     |\n|numeric                  |3     |\n|________________________ |      |\n|Group variables          |None  |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|occupation    |         0|             1|   4|  18|     0|       45|          0|\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts             |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------------|\n|type          |         0|             1|FALSE   |        3|bc: 21, pro: 18, wc: 6 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  mean|    sd| p0| p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|-----:|-----:|--:|---:|---:|---:|----:|:-----|\n|income        |         0|             1| 41.87| 24.44|  7|  21|  42|  64|   81|▇▂▅▃▅ |\n|education     |         0|             1| 52.56| 29.76|  7|  26|  45|  84|  100|▆▆▃▂▇ |\n|prestige      |         0|             1| 47.69| 31.51|  3|  16|  41|  81|   97|▇▃▅▂▇ |\n\n\n:::\n:::\n\n\nLook at the recoded Duncan data\n:::\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n### Explorative Data Analysis {#sec-chap01-5-2}\n\nDuncan used a linear least-squares regression of `prestige` on `income` and `education` to predict the prestige of occupations for which the income and educational scores were known from the U.S. Census but for which there were no direct prestige ratings. He did not use occupational type in his analysis.\n\nA sensible place to start any data analysis, including a regression analysis, is to visualize the data using a variety of graphical displays. We need the following graphs:\n\n- Univariate distributions of the three variables\n- Pairwise or marginal relationships among them\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap01-eda}\n: Explorative Data Analysis (EDA) of Duncan data\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### hist prestige\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-hist-prestige}\n: Histogram of `prestige` variable\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-hist-prestige}\n\n::: {.cell}\n\n```{.r .cell-code}\nd01.1 |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = prestige)\n    ) +\n    ggplot2::geom_histogram(\n        bins = 10,\n        color = \"white\",\n        fill = \"grey40\")\n```\n\n::: {.cell-output-display}\n![](01-getting-started_files/figure-html/hist-prestige-1.png){width=672}\n:::\n:::\n\nHistogram of prestige variable\n:::\n\n***\nThe distribution of `prestige` appears to be bimodal, with cases stacking up near the boundaries, as many occupations are either low prestige, near the lower boundary, or high prestige, near the upper boundary, with relatively fewer occupations in the middle bins of the histogram. Because `prestige` is a percentage, this behavior is not altogether unexpected. Variables such as this often need to be transformed, perhaps with a logit (log-odds) or similar transformation. But transforming `prestige` turns out to be unnecessary in this problem.\n\n::::\n:::::\n\nBefore fitting a regression model to the data, we should also examine the distributions of the predictors education and income, along with the relationship between prestige and each predictor, and the relationship between the two predictors.\n\n###### hist education\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-hist-education}\n: Histogram of `education` variable\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-hist-education}\n\n::: {.cell}\n\n```{.r .cell-code}\nd01.1 |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = education)\n    ) +\n    ggplot2::geom_histogram(\n        bins = 10,\n        color = \"white\",\n        fill = \"grey40\")\n```\n\n::: {.cell-output-display}\n![](01-getting-started_files/figure-html/hist-education-1.png){width=672}\n:::\n:::\n\nHistogram of education variable\n:::\n\n***\n\n\n::::\n:::::\n\n###### hist income\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-hist-income}\n: Histogram of `income` variable\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-hist-income}\n\n::: {.cell}\n\n```{.r .cell-code}\nd01.1 |> \n    ggplot2::ggplot(\n        ggplot2::aes(x = income)\n    ) +\n    ggplot2::geom_histogram(\n        bins = 10,\n        color = \"white\",\n        fill = \"grey40\")\n```\n\n::: {.cell-output-display}\n![](01-getting-started_files/figure-html/hist-income-1.png){width=672}\n:::\n:::\n\nHistogram of income variable\n:::\n\n***\n\n\n::::\n:::::\n\n\n###### scatterplotMatrix\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-scatterplot-matrix}\n: Numbered R Code Title (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-scatterplot-matrix}    \n\n::: {.cell}\n\n```{.r .cell-code}\ncar::scatterplotMatrix(\t~ prestige + education + income, \n                        id = list(n = 3), data = d01.1)\n```\n\n::: {.cell-output-display}\n![](01-getting-started_files/figure-html/scatterplot-matrix-1.png){width=672}\n:::\n:::\n\n\nScatterplot matrix for prestige, education, and income in Duncan’s data, identifying the three most unusual points in each panel. Nonparametric density estimates for the variables appear in the diagonal panels, with a rug-plot (one-dimensional scatterplot) at the bottom of each diagonal panel.\n:::\n\n***\nThe `car::scatterplotMatrix()` function uses a one-sided formula to specify the variables to appear in the graph, where we read the formula `~ prestige + education + income` as “plot prestige and education and income.” \n\nThe argument `id = list(n = 3)` tells `scatterplotMatrix()` to identify the three most unusual points in each panel. This argument was added by the authors after examining a preliminary plot of the data.\n\n::: {.callout-warning #wrn-chap01-difference-in-scatterplot-matrix}\nIn contrast to Figure 1.10 my graph shows the three most unusual points in each panel with a number and not with the name of the occupation. I believe that this difference is a result of my recoding (changing row names into a column).\n:::\n\n::::\n:::::\n\n- **Nonparametric density estimates** are using an adaptive-kernel estimator, and they appear by default in the diagonal panels, with a *rug-plot* (“one-dimensional scatterplot”) at the bottom of each panel, showing the location of the data values for the corresponding variable.\n- The **solid line** shows the marginal linear least-squares fit for the regression of the vertical-axis variable (y) on the horizontal-axis variable (x), ignoring the other variables. \n- The **central broken line** is a nonparametric regression smooth, which traces how the average value of y changes as x changes without making strong assumptions about the form of the relationship between the two variables. \n- The **outer broken lines** represent smooths of the conditional variation of the y values given x in each panel, like running quartiles.\n\n::: {.callout-note #nte-chap01-spm-pairs-ggpairs}\n##### Is there a tidyverse equivalent for car::scatterplotMatrix()?\n\nI believe that `car::scatterplotMatrix()` is a modified `graphics::pairs()` function. `GGally::ggpairs()` is a {**ggplot2**} generalized [pairs plot matrix](https://ggobi.github.io/ggally/articles/ggpairs.html) equivalent in the tidyverse tradition. I should try it out and see if I can reproduce @lst-chap01-scatterplot-matrix with {**GGally**}.\n:::\n\nLike `prestige`, `education` appears to have a bimodal distribution. The distribution of `income`, in contrast, is perhaps best characterized as irregular. The pairwise relationships among the variables seem reasonably linear, which means that as we move from left to right across the plot, the average y values of the points more or less trace out a straight line. The scatter around the regression lines appears to have reasonably constant vertical variability and to be approximately symmetric.\n\nIn addition, two or three cases stand out from the others. In the scatterplot of `income` versus `education`, data point 6 (= ministers) are unusual in combining relatively low income with a relatively high level of education, and data point 16 (= conductors) and data point 27 (= railroad engineers) are unusual in combining relatively high levels of income with relatively low education. Because `education` and `income` are predictors in Duncan’s regression, these three occupations will have relatively high <a class='glossary' title='You can think of the regression line being balanced at the x-mean and the further from that location a point is, the more a single point can move the line. We can measure the distance of points from the mean to quantify each observation’s potential for impact on the line using what is called the leverage of a point. Leverage is a positive numerical measure with larger values corresponding to more leverage. The scale changes depending on the sample size (n) and the complexity of the model so all that matters is which observations have more or less relative leverage in a particular data set. (Outliers - leverage and influence)'>leverage</a> on the regression coefficients. None of these cases, however, are <a class='glossary' title='Outliers are observations with unusual values. (SwR, Glossary). Outliers are observed data points that are far from the least squares line. They have large “errors”, where the “error” or residual is the vertical distance from the line to the point. (Introductory Statistics 12.6)'>outliers</a> in the *univariate* distributions of the three variables.\n\n:::\n\n::::\n:::::\n\n### Regression Analysis {#sec-chap01-5-3}\n\nFollowing Duncan, we next fit a linear least-squares regression to the data to model the joint dependence of prestige on the two predictors, under the assumption that the relationship of prestige to education and income is additive and linear.\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap01-ols-regression}\n: Compute OLS regression\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### fit lm.1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-lm-1}\n: Fit linear model\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-lm-1}\n\n::: {.cell}\n\n```{.r .cell-code}\n(\n    lm01.1 <- stats::lm(\n        formula = prestige ~  education + income,\n        data = d01.1\n    )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> stats::lm(formula = prestige ~ education + income, data = d01.1)\n#> \n#> Coefficients:\n#> (Intercept)    education       income  \n#>     -6.0647       0.5458       0.5987\n```\n\n\n:::\n\n```{.r .cell-code}\nsave_data_file(\"chap01\", lm01.1, \"lm01.1.rds\")\n```\n:::\n\nRegress `prestige` on `education` and `income`\n:::\n\n::::\n:::::\n\n\n###### summary lm.1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-summary-lm-1}\n: Summary of lm01.1\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-summary-lm-1}    \n\n::: {.cell}\n\n```{.r .cell-code}\nbase::summary(lm01.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> stats::lm(formula = prestige ~ education + income, data = d01.1)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -29.538  -6.417   0.655   6.605  34.641 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) -6.06466    4.27194  -1.420    0.163\n#> education    0.54583    0.09825   5.555 1.73e-06\n#> income       0.59873    0.11967   5.003 1.05e-05\n#> \n#> Residual standard error: 13.37 on 42 degrees of freedom\n#> Multiple R-squared:  0.8282,\tAdjusted R-squared:   0.82 \n#> F-statistic: 101.2 on 2 and 42 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nSummary of linear model where `prestige` is regressed on `education` and `income`\n:::\n\n***\n\nThe\t“statistical-significance” asterisks were suppressed with `options(show.signif.stars = FALSE)`.\n\n::::\n:::::\n\nBoth education and income have large regression coefficients in the \"Estimate\" column of the coefficient table, with small two-sided p-values in the column labeled \"Pr (>|t|)\". For example, holding education constant, a 1% increase in higher income earners is associated on average with an increase of about 0.6% in high prestige ratings.\n\n:::\n\n::::\n:::::\n\n### Regression diagnostics {#sec-chap01-5-4}\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap01-regression-diagnostics}\n: Regression diagnostics\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### density\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-density-estimate}\n: Nonparametric density\testimate of the distribution of Studentized residuals\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-density-estimate}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::densityPlot(stats::rstudent(lm01.1))\n```\n\n::: {.cell-output-display}\n![](01-getting-started_files/figure-html/density-estimate-1.png){width=672}\n:::\n:::\n\nNonparametric density estimate for the distribution of the Studentized residuals from the regression of `prestige` on `education` and `income.`\n:::\n\n***\nIf the errors in the regression are normally distributed with zero means and constant variance, then the Studentized residuals are each t-distributed with $n − k − 2$ degrees of freedom, where k is the number of coefficients in the model, excluding the regression constant, and n is the number of cases.\n\n::::\n:::::\n\n\n###### qqPlot\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-qq-plot}\n: Quantile-comparison plot for the Studentized residuals\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-qq-plot}    \n\n::: {.cell}\n\n```{.r .cell-code}\ncar::qqPlot(lm01.1, id = list(n = 3))\n```\n\n::: {.cell-output-display}\n![](01-getting-started_files/figure-html/qq-plot-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1]  6  9 17\n```\n\n\n:::\n:::\n\n\nQuantile-comparison plot for the Studentized residuals from the regression of prestige on education and income. The broken lines show a bootstrapped pointwise 95% confidence envelope for the points.\n:::\n***\n\nThe `car::qqPlot()` function extracts the Studentized residuals and plots them against the quantiles of the appropriate t-distribution. If the Studentized residuals are t-distributed, then the plotted points should lie close to a straight line. The solid comparison line on the plot is drawn by default by robust regression. The argument `id = list (n = 3)` identifies the three most extreme Studentized residuals, and `qqPlot()` returns the (names and) row numbers of these cases.\n\n(In my case the functions returns only the row numbers.)\n\n- 6 : minister\n- 9 : reporter\n- 17: contractor\n\nBy default, `car::qqPlot()` also produces a bootstrapped pointwise 95% confidence envelope for the Studentized residuals that takes account of the correlations among them (but, because the envelope is computed pointwise, does not adjust for simultaneous inference).\n\n::: {.callout-warning #wrn-chap01-bootstrap-waiting-time}\nBe patient! The computation of the bootstrapped pointwise 95% confidence envelope for the points takes some time.\n\nThe bootstrap procedure used by `car::qqPlot()` generates random samples, and so the plot that you see when you duplicate this command will not be identical to the graph shown in the text.\n:::\n\n::::\n:::::\n\nThe residuals stay nearly within the boundaries of the envelope at both ends of the distribution, with the exception of point 6, the occupation `minister`.\n\n###### outlierTest\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-outlier-test}\n: Test based on the largest (absolute) Studentized residual\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-outlier-test}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::outlierTest(lm01.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> No Studentized residuals with Bonferroni p < 0.05\n#> Largest |rstudent|:\n#>   rstudent unadjusted p-value Bonferroni p\n#> 6 3.134519          0.0031772      0.14297\n```\n\n\n:::\n:::\n\n\nOutlier test: Studentized residuals with Bonferroni\n:::\n\n***\n\nThe outlier test suggests that the residual for ministers is not terribly unusual, with a Bonferroni-corrected p-value of 0.14.\n\n::::\n:::::\n\n###### influenceIndexPlot()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-influenctial-cases}\n: Checking for high-leverage and influential cases \n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-influenctial-cases}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::influenceIndexPlot(\n    model = lm01.1, \n    vars = c(\"Cook\", \"hat\"),\n    id = list(n = 3)\n    )\n```\n\n::: {.cell-output-display}\n![](01-getting-started_files/figure-html/influenctial-cases-1.png){width=960}\n:::\n:::\n\n\nIndex plots of <a class='glossary' title='Cook’s distance (often abbreviated Cook’s D) is used in Regression Analysis to find influential outliers in a set of predictor variables. IIt is a way to identify points that negatively affect the regression model. (Statistics How To)'>Cook’s distances</a> and <a class='glossary' title='The term ‘y hat’ (written as ŷ) refers to the estimated value of a response variable in a linear regression model.(What is Y Hat in Statistics?)'>hat-values</a>, from the regression of `prestige` on `income` and `education.`\n:::\n\n\n\n::::\n:::::\n\nBecause the cases in a regression can be *jointly* as well as individually influential, we also examine *added-variable plots* for the predictors, using `car::avPlots()`.\n\n###### avPlots\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-av-plots}\n: Added-variable plots\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-av-plots}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::avPlots(\n    model = lm01.1,\n    id = list(\n        cex = 0.75,\n        n = 3,\n        method = \"mahal\"\n        )\n)\n```\n\n::: {.cell-output-display}\n![](01-getting-started_files/figure-html/av-plots-1.png){width=672}\n:::\n:::\n\n\nAdded-variable plots for education and income in Duncan’s occupational-prestige regression\n:::\n\n***\n\nThe `id` argument, which has several components here, customizes identification of points in the graph:\n\n- `cex = 0.75` (where cex is a standard R argument for “character expansion”) makes the labels smaller, so that they fit better into the plots\n- `n = 3` identifies the three most unusual points in each plot\n- `method = \"mahal\"` indicates that unusualness is quantified by Mahalanobis distance from the center of the point-cloud.\n\nMahalanobis\tdistances\tfrom\tthe\tcenter\tof\tthe\tdata\ttake\taccount\tof\tthe standard\tdeviations\tof\tthe\tvariables\tand\tthe\tcorrelation\tbetween\tthem. Each\tadded-variable\tplot\tdisplays\tthe\t*conditional*,\trather\tthan\tthe\tmarginal, relationship\tbetween\tthe\tresponse\tand\tone\tof\tthe\tpredictors.\tPoints\tat\tthe extreme\tleft\tor\tright\tof\tthe\tplot\tcorrespond\tto\tcases\tthat\thave\thigh\tleverage\ton the\tcorresponding\tcoefficient\tand\tconsequently\tare\tpotentially\tinfluential.\n\n::::\n:::::\n\n@lst-chap01-av-plots confirms\tand\tstrengthens\tour\tprevious\tobservations:\tWe\tshould\tbe concerned\tabout\tthe\toccupations\tminister (point 6)\tand\tconductor (point 16),\twhich\twork\tjointly\tto increase\tthe\teducation\tcoefficient\tand\tdecrease\tthe\tincome\tcoefficient. Occupation\tRR.engineer (point 27)\thas\trelatively\thigh\tleverage\ton\tthese\tcoefficients\tbut\tis more\tin\tline\twith\tthe\trest\tof\tthe\tdata.\n\n###### crPlots\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-cr-plots}\n: Component-plus-residual plots\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-cr-plots}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::crPlots(lm01.1)\n```\n\n::: {.cell-output-display}\n![](01-getting-started_files/figure-html/cr-plots-1.png){width=672}\n:::\n:::\n\n\nComponent-plus-residual\tplots\tfor\teducation\tand\tincome\tin Duncan’s\toccupational-prestige\tregression.\tThe\tsolid\tline\tin\teach\tpanel\tshows\ta loess\tnonparametric-regression\tsmooth;\tthe\tbroken\tline\tin\teach\tpanel\tis\tthe least-squares\tline\n:::\n\n***\n\nEach\tplot\tincludes\ta least-squares\tline,\trepresenting\tthe\tregression\tplane\tviewed\tedge-on\tin\tthe direction\tof\tthe\tcorresponding\tpredictor,\tand\ta\tloess\tnonparametric-regression smooth.\t\n\n::::\n:::::\n\nThe\tpurpose\tof\t@lst-chap01-cr-plots\tis\tto\tdetect\tnonlinearity,\tevidence\tof\twhich is\tslightly\there.\n\n###### ncvTest\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-ncv-test}\n: Score\ttests for nonconstant variance\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-ncv-test}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::ncvTest(lm01.1)\n\nglue::glue(\" \")\n\ncar::ncvTest(\n    model = lm01.1,\n    var.formula = ~ income + education\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Non-constant Variance Score Test \n#> Variance formula: ~ fitted.values \n#> Chisquare = 0.3810967, Df = 1, p = 0.53702\n#>  \n#> Non-constant Variance Score Test \n#> Variance formula: ~ income + education \n#> Chisquare = 0.6976023, Df = 2, p = 0.70553\n```\n\n\n:::\n:::\n\n\nChecking\tfor\tan\tassociation\tof\tresidual variability\twith\tthe\tfitted\tvalues\tand\twith\tany\tlinear\tcombination\tof\tthe predictors\n:::\n\n***\nBoth\ttests\tyield\tlarge\tp-values,\tindicating\tthat\tthe\tassumption\tof\tconstant variance\tis\ttenable\n\n\n\n::::\n:::::\n\n###### fit-lm2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-regress-lm01.2}\n: Linear regression without influential data of 6 and 16\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-regress-lm01.2}\n\n::: {.cell}\n\n```{.r .cell-code}\n d01.2 <- d01.1 |> \n        dplyr::slice(-c(6, 16))\n \nlm01.2 <- stats::update(lm01.1, subset = -c(6, 16))\n\nbase::summary(lm01.2)\n\nsave_data_file(\"chap01\", d01.2, \"d01.2.rds\")\nsave_data_file(\"chap01\", lm01.2, \"lm01.2.rds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> stats::lm(formula = prestige ~ education + income, data = d01.1, \n#>     subset = -c(6, 16))\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -28.612  -5.898   1.937   5.616  21.551 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) -6.40899    3.65263  -1.755   0.0870\n#> education    0.33224    0.09875   3.364   0.0017\n#> income       0.86740    0.12198   7.111 1.31e-08\n#> \n#> Residual standard error: 11.42 on 40 degrees of freedom\n#> Multiple R-squared:  0.876,\tAdjusted R-squared:  0.8698 \n#> F-statistic: 141.3 on 2 and 40 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nLinear model version 2 without influential data points 6 and 16\n:::\n***\n\nRather\tthan\trespecifying\tthe\tregression\tmodel\tfrom\tscratch\twith\t`stats::lm()`,\twe\trefit\tit using\tthe\t`stats::update()`\tfunction,\tremoving\tthe\ttwo\tpotentially\tproblematic\tcases\tvia the\tsubset\targument to\t`update()`.\t\n\nI didn't need `car::whichNames()` to get the row numbers to\tbe\tremoved, because I used these numbers already all the time. \n\n::::\n:::::\n\n###### compareCoefs-1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-compare-coefs}\n: Comparing\tthe\testimated coefficients of lm01.1 and lm01.2\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-compare-coefs}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::compareCoefs(lm01.1, lm01.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Calls:\n#> 1: stats::lm(formula = prestige ~ education + income, data = d01.1)\n#> 2: stats::lm(formula = prestige ~ education + income, data = d01.1, subset =\n#>    -c(6, 16))\n#> \n#>             Model 1 Model 2\n#> (Intercept)   -6.06   -6.41\n#> SE             4.27    3.65\n#>                            \n#> education    0.5458  0.3322\n#> SE           0.0983  0.0987\n#>                            \n#> income        0.599   0.867\n#> SE            0.120   0.122\n#> \n```\n\n\n:::\n:::\n\n\nComparing\tthe\testimated\tcoefficients\tand\ttheir\tstandard\terrors\tacross\tthe\ttwo regressions\tfit\tto\tthe\tdata\n:::\n\n***\n\nThe\tcoefficients\tof\teducation\tand\tincome\tchanged\tsubstantially\twith\tthe\tdeletion of\tthe\toccupations\tminister\tand\tconductor.\tThe\teducation\tcoefficient\tis considerably\tsmaller\tand\tthe\tincome\tcoefficient\tconsiderably\tlarger\tthan\tbefore.\n\n::::\n:::::\n\n###### fit-lm3\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-regress-lm01.3}\n: Linear regression without influential data of 6, 9, 16, and 27\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-regress-lm01.3}\n\n::: {.cell}\n\n```{.r .cell-code}\n d01.3 <- d01.1 |> \n        dplyr::slice(-c(6, 9, 16, 27))\n \nlm01.3 <- stats::update(lm01.1, subset = -c(6, 9, 16, 27))\n\nbase::summary(lm01.3)\n\nsave_data_file(\"chap01\", d01.3, \"d01.3.rds\")\nsave_data_file(\"chap01\", lm01.3, \"lm01.3.rds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> stats::lm(formula = prestige ~ education + income, data = d01.1, \n#>     subset = -c(6, 9, 16, 27))\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -25.348  -5.376   1.775   4.498  20.411 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)  -7.1472     3.4075  -2.097   0.0427\n#> education     0.3019     0.1121   2.692   0.0105\n#> income        0.9465     0.1420   6.668 6.94e-08\n#> \n#> Residual standard error: 10.6 on 38 degrees of freedom\n#> Multiple R-squared:  0.8973,\tAdjusted R-squared:  0.8919 \n#> F-statistic:   166 on 2 and 38 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nLinear model version 3 without influential data points 6, 9, 16, and 27.\n:::\n\n::::\n:::::\n\n###### compareCoefs-2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-compare-coefs2}\n: Comparing\tthe\testimated coefficients of lm01.1 and lm01.3\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-compare-coefs2}\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::compareCoefs(lm01.2, lm01.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Calls:\n#> 1: stats::lm(formula = prestige ~ education + income, data = d01.1, subset =\n#>    -c(6, 16))\n#> 2: stats::lm(formula = prestige ~ education + income, data = d01.1, subset =\n#>    -c(6, 9, 16, 27))\n#> \n#>             Model 1 Model 2\n#> (Intercept)   -6.41   -7.15\n#> SE             3.65    3.41\n#>                            \n#> education    0.3322  0.3019\n#> SE           0.0987  0.1121\n#>                            \n#> income        0.867   0.947\n#> SE            0.122   0.142\n#> \n```\n\n\n:::\n:::\n\n\nComparing\tthe\testimated\tcoefficients\tand\ttheir\tstandard\terrors\tacross\tthe\ttwo regressions\tfit\tto\tthe\tdata\n:::\n\n***\n\nThe\tcoefficients\tof\teducation\tand\tincome\tchanged not\tsubstantially\twith\tthe\tadditional deletion of\tthe\toccupations\treporter (9) and RR.engineer (27).\tThe\teducation\tcoefficient in `lm01.3`\tis only somewhat smaller\tand\tthe\tincome\tcoefficient is only\ta little\tlarger\tthan with `lm01.2`.\n\n::::\n:::::\n:::\n\n\n\n\n::::\n:::::\n\n## R Functions for Basic Statistics (empty) {#sec-chap01-6}\n\n## Generic Functions and Their Methods {#sec-chap01-7}\n\n\n::: {.callout-note #nte-chap01-generic-functions}\n##### Generic function and their methods: An important explanation for me\n\nThe following text is almost completely copied from the R companion. It explains specific structures of the R language I already met in different situations but did not understand completely. This has changed now thanks to the explication in the R companion book.\n:::\n\n\n\n\n\nMany\tof\tthe\tmost\tcommonly\tused\tfunctions\tin\tR,\tsuch\tas\t`summary()`,\t`print()`, and\t`plot()`,\tproduce\tdifferent\tresults\tdepending\ton\tthe\targuments\tpassed\tto\tthe function. Enabling\tthe\tsame\t*generic\tfunction*,\tsuch\tas\t`summary()`,\tto\tbe\tused\tfor\tmany purposes\tis\taccomplished\tin\tR\tthrough\tan\t*object-oriented\tprogramming technique*\tcalled\t*object\tdispatch*.\n\nThe\tdetails\tof\tobject\tdispatch\tare\timplemented differently\tin\tthe\tS3\tand\tS4\tobject\tsystems,\tso\tnamed\tbecause\tthey\toriginated\tin Versions\t3\tand\t4,\trespectively,\tof\tthe\toriginal\tS\tlanguage\ton\twhich\tR\tis\tbased. There\tis\tyet\tanother\timplementation\tof\tobject\tdispatch\tin\tR\tfor\tthe\tmore\trecently introduced\tsystem\tof\treference\tclasses\t(sometimes\tcolloquially\ttermed\t“R5”). Almost\teverything\tcreated\tin\tR\tis\tan\tobject,\tsuch\tas\ta\tnumeric\tvector,\ta\tmatrix,\ta data\tframe,\ta\tlinear\tregression\tmodel,\tand\tso\ton.\tIn\tthe\tS3\tobject\tsystem, described\tin\tthis\tsection\tand\tused\tfor\tmost\tR\tobject-oriented\tprograms,\teach object\tis\tassigned\ta\tclass,\tand\tit\tis\tthe\tclass\tof\tthe\tobject\tthat\tdetermines\thow generic\tfunctions\tprocess\tthe\tobject.\tWe\twon’t\ttake\tup\tthe\tS4\tand\treference-class\tobject\tsystems\tin\tthis\tbook,\tbut\tthey\ttoo\tare\tclass\tbased\tand\timplement (albeit\tmore\tcomplex)\tversions\tof\tobject\tdispatch.\n\nGeneric\tfunctions\toperate\ton\ttheir\targuments\tindirectly\tby\tcalling\tspecialized functions,\treferred\tto\tas\tmethod\tfunctions\tor,\tmore\tcompactly,\tas\tmethods. Which\tmethod\tis\tinvoked\ttypically\tdepends\ton\tthe\tclass\tof\tthe\tfirst\targument\tto the\tgeneric\tfunction.\n\nIn\tcontrast,\tin\tthe\tS4\tobject\tsystem,\tmethod\tdispatch\tcan\tdepend\ton\tthe classes\tof\tmore\tthan\tone\targument\tto\ta\tgeneric\tfunction. For\texample,\tthe\tgeneric\t`summary()`\tfunction\thas\tthe\tfollowing\tdefinition: \n\n```\n- **summary** \n- function\t(object,\t...) \n- UseMethod\t(\"summary\") \n-<bytecode:\t0x000000001d03ba78> \n- <environment:\tnamespace:base> \n```\nThe\tgeneric\tfunction\t`summary()`\thas\tone\trequired\targument,\tobject, and\tthe\tspecial\targument\t`...`\t(the\tellipses)\tfor\tadditional\targuments\tthat\tcould vary\tfrom\tone\t`summary\t()`\tmethod\tto\tanother.\n\nWhen\tUseMethod\t(\"summary\")\tis\tcalled\tby\tthe\t`summary()`\tgeneric,\tand\tthe\tfirst (object)\targument\tto\t`summary()`\tis\tof\tclass\t\"lm\",\tfor\texample,\tR\tsearches\tfor\ta method\tfunction\tnamed\t`summary.lm()`,\tand,\tif\tit\tis\tfound,\texecutes\tthe command\t`summary.lm(object,\t...)`.\tIt\tis,\tincidentally,\tperfectly\tpossible\tto\tcall `summary.lm()`\tdirectly.\n\n\nAlthough\tthe\tgeneric\t`summary()`\tfunction\thas\tonly\tone\texplicit\targument,\tthe method\tfunction\t`summary.lm()`\thas\tadditional\targuments: \n\n```\n- args(\"summary.lm\") \n- function(object,\tcorrelation\t=\tFALSE,\tsymbolic.cor\t=\tFALSE, ...) \n- NULL\n```\n\nBecause\tthe\targuments\t`correlation`\tand\t`symbolic.cor`\thave\tdefault\tvalues (FALSE,\tin\tboth\tcases),\tthey\tneed\tnot\tbe\tspecified.\tThus,\tfor\texample,\tif\twe enter\tthe\tcommand\t`summary(lm01.1,\tcorrelation=TRUE)`,\tthe\targument `correlation=TRUE`\tis\tabsorbed\tby\t`...`\tin\tthe\tcall\tto\tthe\tgeneric\t`summary()` function\tand\tthen\tpassed\tto\tthe\t`summary.lm()`\tmethod,\tcausing\tit\tto\tprint\ta correlation\tmatrix\tfor\tthe\tcoefficient\testimates.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-generic-function-additional-argument}\n: Example of the generic `summary()` function with additional argument\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-generic-function-additional-argument}\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary.lm(lm01.1,\tcorrelation = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> stats::lm(formula = prestige ~ education + income, data = d01.1)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -29.538  -6.417   0.655   6.605  34.641 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) -6.06466    4.27194  -1.420    0.163\n#> education    0.54583    0.09825   5.555 1.73e-06\n#> income       0.59873    0.11967   5.003 1.05e-05\n#> \n#> Residual standard error: 13.37 on 42 degrees of freedom\n#> Multiple R-squared:  0.8282,\tAdjusted R-squared:   0.82 \n#> F-statistic: 101.2 on 2 and 42 DF,  p-value: < 2.2e-16\n#> \n#> Correlation of Coefficients:\n#>           (Intercept) education\n#> education -0.36                \n#> income    -0.30       -0.72\n```\n\n\n:::\n:::\n\n\nGeneric function `summary()` with additional argument\n:::\n\n::::\n:::::\n\nIn\tthis\tinstance,\twe\tcan\tcall\t`summary.lm()`\tdirectly,\tbut\tmost\tmethod\tfunctions are\thidden\tin\t(not\t“exported\tfrom”)\tthe\tnamespaces\tof\tthe\tpackages\tin\twhich\tthe methods\tare\tdefined\tand\tthus\tcannot\tnormally\tbe\tused\tdirectly.\tIn\tany\tevent,\tit is\tgood\tR\tform\tto\tuse\tmethod\tfunctions\tonly\tindirectly\tthrough\ttheir\tgenerics.\t\n\nFor\texample,\tthe\t`summary()`\tmethod\t`summary.boot()`,\tfor\tsummarizing\tthe results\tof\tbootstrap\tresampling\tis\thidden\tin\tthe\tnamespace\tof the\t{**car**}\tpackage.\tTo\tcall\tthis\tfunction\tdirectly\tto\tsummarize\tan\tobject\tof\tclass \"boot\",\twe\tcould\treference\tthe\tfunction\twith\tthe\tunintuitive\tpackage-qualified name\t`car:::summary.boot()`,\tbut\tcalling\tthe\tunqualified\tmethod\t`summary.boot()` directly\twon’t\twork.\n\nSuppose\tthat\twe\tinvoke\tthe\thypothetical\tgeneric\tfunction\t`fun()`,\tdefined\tas: \n\n```\nfun\t<-\tfunction(x,\t...){ \n    UseMethod\t(\"fun\") \n    } \n``` \n\nwith\treal\targument\tobj\tof\tclass\t\"cls\":\t`fun\t(obj)`.\tIf\tthere\tis\tno\tmethod\tfunction named\t`fun.cls()`,\tthen\tR\tlooks\tfor\ta\tmethod\tnamed\t`fun.default()`.\tFor\texample, objects\tbelonging\tto\tclasses\twithout\t`summary()`\tmethods\tare\tsummarized\tby `summary.default()`.\tIf,\tunder\tthese\tcircumstances,\tthere\tis\tno\tmethod\tnamed `fun.default()`,\tthen\tR\treports\tan\terror.\n\nWe\tcan\tget\ta\tlisting\tof\tall\tcurrently\taccessible\tmethods\tfor\tthe\tgeneric\t`summary()`\tfunction\tusing\tthe\t`utils::methods()`\tfunction,\twith\thidden\tmethods\tflagged\tby asterisks.\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap01-generic-functions}\n: Generic functions and their methods\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### methods1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-list-methods}\n: Listing for all the methods for a generic function\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-list-methods}\n\n::: {.cell}\n\n```{.r .cell-code}\nutils::methods(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1] summary.Anova.mlm*                  summary.aov                        \n#>  [3] summary.aovlist*                    summary.aspell*                    \n#>  [5] summary.bcnPowerTransform*          summary.bcnPowerTransformlmer*     \n#>  [7] summary.boot*                       summary.check_packages_in_dir*     \n#>  [9] summary.connection                  summary.data.frame                 \n#> [11] summary.Date                        summary.default                    \n#> [13] summary.ecdf*                       summary.factor                     \n#> [15] summary.ggplot*                     summary.glm                        \n#> [17] summary.hcl_palettes*               summary.infl*                      \n#> [19] summary.lm                          summary.loess*                     \n#> [21] summary.manova                      summary.matrix                     \n#> [23] summary.mlm*                        summary.nls*                       \n#> [25] summary.packageStatus*              summary.POSIXct                    \n#> [27] summary.POSIXlt                     summary.powerTransform*            \n#> [29] summary.ppr*                        summary.prcomp*                    \n#> [31] summary.princomp*                   summary.proc_time                  \n#> [33] summary.rlang_error*                summary.rlang_message*             \n#> [35] summary.rlang_trace*                summary.rlang_warning*             \n#> [37] summary.rlang:::list_of_conditions* summary.skim_df*                   \n#> [39] summary.srcfile                     summary.srcref                     \n#> [41] summary.stepfun                     summary.stl*                       \n#> [43] summary.table                       summary.tukeysmooth*               \n#> [45] summary.vctrs_sclr*                 summary.vctrs_vctr*                \n#> [47] summary.warnings                   \n#> see '?methods' for accessing help and source code\n```\n\n\n:::\n:::\n\n\nList all the methods for the generic function `summary()`\n\n:::\n\n::::\n:::::\n\nYou\tcan\talso\tdetermine\twhat\tgeneric\tfunctions\thave\tcurrently\tavailable\tmethods for\tobjects\tof\ta\tparticular\tclass.\n\n###### methods-class1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-list-class-methods}\n: List all available methods for a specific class\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-list-class-methods}  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nutils::methods(class = \"lm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1] add1           alias          anova          case.names     coerce        \n#>  [6] confint        cooks.distance deviance       dfbeta         dfbetas       \n#> [11] drop1          dummy.coef     effects        extractAIC     family        \n#> [16] formula        hatvalues      influence      initialize     kappa         \n#> [21] labels         logLik         model.frame    model.matrix   nobs          \n#> [26] plot           predict        print          proj           qr            \n#> [31] residuals      rstandard      rstudent       show           simulate      \n#> [36] slotsFromS3    summary        variable.names vcov          \n#> see '?methods' for accessing help and source code\n```\n\n\n:::\n:::\n\n\nList all available methods for a class = \"lm\"\n:::\n\n::::\n:::::\n\nIn the above code chunk I have not loaded the {**car**} package which has many other methods for class \"lm\". (See next tab)\n\n###### methods-class2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-list-class-methods2}\n: List all available methods for a specific class with additional package loaded\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-list-class-methods2}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Loading required package: carData\n```\n\n\n:::\n\n```{.r .cell-code}\nutils::methods(class = \"lm\")\ndetach(\"package:car\", unload = TRUE)\ndetach(\"package:carData\", unload = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1] add1                alias               anova              \n#>  [4] Anova               avPlot              avPlot3d           \n#>  [7] Boot                bootCase            boxCox             \n#> [10] brief               case.names          ceresPlot          \n#> [13] coerce              confidenceEllipse   confint            \n#> [16] Confint             cooks.distance      crPlot             \n#> [19] crPlot3d            deltaMethod         deviance           \n#> [22] dfbeta              dfbetaPlots         dfbetas            \n#> [25] dfbetasPlots        drop1               dummy.coef         \n#> [28] durbinWatsonTest    effects             extractAIC         \n#> [31] family              formula             hatvalues          \n#> [34] hccm                infIndexPlot        influence          \n#> [37] influencePlot       initialize          inverseResponsePlot\n#> [40] kappa               labels              leveneTest         \n#> [43] leveragePlot        linearHypothesis    logLik             \n#> [46] mcPlot              mmp                 model.frame        \n#> [49] model.matrix        ncvTest             nextBoot           \n#> [52] nobs                outlierTest         plot               \n#> [55] powerTransform      predict             Predict            \n#> [58] print               proj                qqPlot             \n#> [61] qr                  residualPlot        residualPlots      \n#> [64] residuals           rstandard           rstudent           \n#> [67] S                   show                sigmaHat           \n#> [70] simulate            slotsFromS3         spreadLevelPlot    \n#> [73] summary             symbox              variable.names     \n#> [76] vcov                vif                \n#> see '?methods' for accessing help and source code\n```\n\n\n:::\n:::\n\n\nExample of using the `methods()` function with the {**car**} package loaded\n:::\n\n\n::::\n:::::\n\nMethod\tselection\tis\tslightly\tmore\tcomplicated\tfor\tobjects\twhose\tclass\tis\ta\tvector of\tmore\tthan\tone\telement. Consider,\tfor\texample,\tan\tobject\treturned\tby\tthe\t`glm()`\tfunction\tfor\tfitting\tgeneralized\tlinear\tmodels\t(anticipating\ta\tlogistic-regression\texample).\n\n###### methods-class3\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap01-list-class-methods3}\n: List all available methods for a specific vector class\n::::::\n:::\n::::{.my-r-code-container}\n::: {#lst-chap01-list-class-methods3}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Loading required package: carData\n```\n\n\n:::\n\n```{.r .cell-code}\nmod.mroz <-\tstats::glm(\n    lfp\t~ ., \n    family = binomial, \n    data = Mroz)\nclass(mod.mroz)\n\ndetach(\"package:car\", unload = TRUE)\ndetach(\"package:carData\", unload = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"glm\" \"lm\"\n```\n\n\n:::\n:::\n\n\nList all available methods for a voectorized class \n:::\n\n\n::::\n:::::\n\nThe\t`.`\ton\tthe\tright-hand\tside\tof\tthe\tmodel\tformula\tindicates\tthat\tthe\tresponse variable\t`lfp`\tis\tto\tbe\tregressed\ton\tall\tof\tthe\tother\tvariables\tin\tthe\t`Mroz`\tdata\tset (which\tis\taccessible\tbecause\tit\tresides\tin\tthe\t{**carData**}\tpackage). If\twe\tinvoke\ta\tgeneric\tfunction\twith\t`mod.mroz`\tas\tits\targument,\tsay\t`fun(mod. mroz)`,\tthen\tthe\tR\tinterpreter\twill\tlook\tfirst\tfor\ta\tmethod\tnamed\t`fun.glm()`.\tIf\ta function\tby\tthis\tname\tdoes\tnot\texist,\tthen\tR\twill\tsearch\tnext\tfor\t`fun.lm()`\tand finally\tfor\t`fun.default()`.\tWe\tsay\tthat\tthe\tobject\t`mod.mroz`\tis\tof\t*primary\tclass* \"glm\"\tand\t*inherits*\tfrom\tclass\t\"lm\".\n\n:::\n\n::::\n:::::\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}