# Fitting linear models {#sec-chap04}


```{r}
#| label: setup
#| results: hold
#| include: false

base::source(file = "R/helper.R")
ggplot2::theme_set(ggplot2::theme_bw())
options(show.signif.stars = FALSE)

```

## Table of content for chapter 04 {.unnumbered}


::::: {#obj-chap04}
:::: {.my-objectives}
::: {.my-objectives-header}
Chapter section list
:::

::: {.my-objectives-container}

- The linear model (@sec-chap04-1)
- Linear least-square regression (@sec-chap04-2)
    - Simple linear regression (@sec-chap04-1)



:::
::::
:::::

## The linear model {#sec-chap04-1}

:::{.my-bulletbox}
:::: {.my-bulletbox-header}
::::: {.my-bulletbox-icon}
:::::
:::::: {#bul-chap04-lm-assumptions}
::::::
: Assumption of the linear model
::::
:::: {.my-bulletbox-body}
The assumptions of the linear models are:

- **Continuous variable**: A numeric variable that is at least nominally continuous. (Qualitative response variables and count response variables require other regression models. See @sec-chap06.)
- **Independence**: The observations of the variables for one case are independent of the observations for all other cases. (If cases are dependent, linear mixed-effects models may be more appropriate. See @sec-chap07.)
- **Linearity**: The dependence of the response on the predictors is through the conditional expected value or *mean function*. (The {**car**} package contains many functions that can help you decide whether the assumption of linearity is reasonable in any given problem. See @sec-chap08.)
- **Constant conditicional variance**: The conditional variance of the response given the regressors (or, equivalently, the predictors) is constant. (See @sec-chap05-1 --- @sec-chap05-3 and @sec-chap08-5 for diagnostics tools.)

***

Changing assumptions changes the model. For example, it is common to add a normality assumption, producing the *normal linear model*. Another common extension to the linear model is to modify the constant variance assumption producing the *weighted linear model*. 

::::
:::


## Linear least-square regression {#sec-chap04-2}

### Simple linear regression {#sec-chap04-2-1}

The Davis data set in the {**carData**} package contains the measured and selfreported heights and weights of 200 men and women engaged in regular exercise. A few of the data values are missing, and consequently there are only 183 complete cases for the variables that are used in the analysis reported below.

The variables `weight` (measured weight) and `repwt` (reported weight) are in kilograms, and `height` (measured height) and `repht` (reported height) are in centimeters. We focus here on the regression of `weight` on `repwt.`

:::::{.my-example}
:::{.my-example-header}
:::::: {#exm-chap04-lm-davis}
: Simple linear regression 
::::::
:::
::::{.my-example-container}

::: {.panel-tabset}

###### Davis

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-skim-Davis-data}
: A quick look at the Davis data
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-skim-Davis-data}
```{r}
#| label: skim-Davis-data

Davis <- carData::Davis

skimr::skim(Davis)
```

Skim the Davis data set
:::

***

Instead of using the `base::summary()` function I have applied the `skim()` function from the {**skimr**} package. See @pak-skimr.

::::
:::::

###### davis_lm0

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-print-davis-lm0}
: Simple linear regression of Davis data
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-print-davis-lm0}
```{r}
#| label: print-davis-lm0
#| results: hold

glue::glue("Standard call of the lm function")
(davis_lm0 <- lm(weight ~ repwt, data = Davis))

save_data_file("chap04", davis_lm0, "davis_lm0.rds")

glue::glue("")
glue::glue("################################################################")
glue::glue("Suppressing the intercept to force the regression through the origin")
lm(weight ~ -1 + repwt, data = Davis)
```
Simple	linear	regression	model of the Davis data
:::

***

A model formula consists of three parts: 
 
- 1. **The left-hand side** of the model formula specifies the response variable; it is usually a variable name (`weight`, in the example) but may be an expression that evaluates to the response (e.g., `sqrt(weight)`, `log(income)`, or `income/hours.worked`). 
- 2. **The tilde** is a separator. 
- 3. **The right-hand side** is the most complex part of the formula. It is a special expression, including the names of the predictors, that R evaluates to produce the regressors for the model. **The arithmetic operators, +, -, *, /, and ^, have special meaning on the right-hand side of a model formula, but they retain their ordinary meaning on the left-hand side of the formula.** R will use any unadorned numeric predictor on the right-hand side of the model formula as a regressor, as is desired here for simple regression. The intercept $\beta_{0}$ is included in the model without being specified directly.

We can suppress the intercept to force the regression through the origin using `weight ~ -1 + repwt` or `weight ~ repwt - 1`. More generally, a minus sign removes a term, here the intercept, from the predictor. Using $0$ (zero) also suppresses the intercept. That $0$ is an equivalent of $-1$ can be seen in the repetition of the formula which uses $0$ although I have used in the formula $-1$.

The `stats::lm()` function returns a linear-model object, which was assigned to the R variable `davis_lm0`. Printing this object just results in a relatively sparse output: Printing a linear-model object simply shows the command that produced the model along with the estimated regression coefficients.

::: {.callout-caution style="color: darkgoldenrod;" #cau-chap04-without-intercept}
##### Difference in coefficient without intercept

There is a small difference in the coefficient with and without intercept. Where does this come from and what does it mean?

Is this the result "to force the regression through the origin"?
:::


::::
:::::



###### stats::summary()

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-summary-davis-lm0}
: Summary of the linear model `davis_lm0`
::::::
:::
::::{.my-r-code-container}
::: {#lst-listing-ID}    
```{r}
#| label: summary-davis-lm0

summary(davis_lm0)
```

Using stats::summary() for the `davis_lm0` object
:::

***

For an explication of the different printed parts see @lst-chap04-car-S-davis-lm0.

::::
:::::

###### car::S()

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-car-S-davis-lm0}
: Default usage of the more sophisticated summary with `car::S()`
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-car-S-davis-lm0}
```{r}
#| label: S-davis-lm0

car::S(davis_lm0)
```

Default summary output of `car::S()`
:::

***

As a subset of the stats:summary() printout it does not report the residual distribution but adds  with AIC and BIC two diagnostics figures for compare and choose between different models.

- **Call**: A brief header is printed first, repeating the command that created the regression model. 
- **Coefficients**: The part of the output marked `Coefficients` provides basic information about the estimated regression coefficients. 
    - The **row names** (=first column) lists the names of the regressors fit by `lm()`. 
        - **Intercept**: The intercept, if present, is named `(Intercept)`. 
        - **Estimate**: The column labeled `Estimate` provides the least-squares estimates of the regression coefficients. 
        - **Std. Error**: The column marked `Std. Error` displays the standard errors of the estimated coefficients. The default standard errors are computed as $\sigma$ times a function of the regressors, as presented in any regression text, but the `car::S()` function also allows you to use other methods for computing standard errors. 
        - **t-value**: The column marked `t value` is the ratio of each estimate to its standard error and is a `r glossary("Wald", "Wald test statistic")` for the null hypothesis that the corresponding population regression coefficient is equal to zero. If assumptions hold and the errors are normally distributed or the sample size is large enough, then these `r glossary("t-statistic", "t-statistics")` computed with the default `r glossary("standard error")` estimates are distributed under the null hypothesis as t random variables with `r glossary("degrees of freedom")` (df ) equal to the `r glossary("Residually", "residual")` degrees of freedom under the model. 
        - **Pr (>|t|)**: The column marked `Pr (>|t|)` is the two-sided `r glossary("p-value")` for the null hypothesis assuming that $\beta_{0} = 0$	versus	$\beta_{0} \neq 0$ the t-distribution is appropriate. (*Example*: The hypothesis $\beta_{0} = 0$	versus	$\beta_{0} != 0$, with the value of $\beta_{1}$ unspecified, has a p-value of about .08, providing weak evidence against the null hypothesis that $\beta_{0} = 0$, if the assumptions of the model hold.)
    - Below the coefficient table is additional summary information, including the **residual standard deviation**. (*Example*: This typical error in prediction is so large that, if correct, it is unlikely that the predictions of actual weight from reported weight would be of any practical value.)
    - The **residual df** are n − 2 for simple regression, here 183 − 2 = 181, and we’re alerted to the fact that 17 cases are removed because of missing data. 
    - The **Multiple R-squared**, $R^2 \approx 0.70$, is the square of the correlation between the response and the fitted values and is interpretable as the proportion of variation of the response variable around its mean accounted for by the regression. 
    - **F-statistic**: The reported `r glossary("F-statistic")` provides a `r glossary("likelihood-ratio")` test of the general null hypothesis that all the population regression coefficients, except for the intercept, are equal to zero, versus the alternative that at least one of the $\beta_{j}$ is nonzero. If the errors are normal or n is large enough, then this test statistic has an F-distribution with the degrees of freedom shown. Because simple regression has only one parameter beyond the intercept, the F-test is equivalent to the t-test that $\beta_{1} = 0$, with $t_{2} = F$. (In other models, such as GLMs or normal nonlinear models, Wald tests and the generalization of the likelihood-ratio F-test may test the same hypotheses, but they need not provide the same inferences.) 
    - The **AIC** and **BIC** values at the bottom of the output are, respectively, the Akaike Information Criterion and the Bayesian Information Criterion, statistics that are sometimes used for model selection.

::::
:::::

::: {.callout-note style="color: blue;" #nte-chap04-S-versus-summary}
With the exception of the AIC and BIC values I cannot see any advantage in the default version of `car::S()` against `base::summary()`. I am curious what it means that car::S() "adds more functionality".

One obviously is, that car::S() allows different methods to compute standard errors.
:::



###### car::brief()

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-car-brief-davis-lm0}
: Short summary of the lm object with `car::brief()`
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-car-brief-davis-lm0}
```{r}
#| label: car-brief-davis-lm0

car::brief(davis_lm0)
```

Brief summary of the lm object with `car::brief()`
:::

car::brief() for a `lm` object prints only the first two columns of the coefficients: `Estimate` and `Std. Error`.

::::
:::::

###### Confidence intervals

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-confidence-intervals-davis-lm0}
: Confidence intervals for the estimates of `davis_lm0`
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-confidence-intervals-davis-lm0}
```{r}
#| label: confidence-intervals-davis-lm0
#| results: hold

glue::glue("Confidence intervals with `stats::confint()`")
stats::confint(davis_lm0)
glue::glue(" ")
glue::glue("#############################################")
glue::glue("Cofidence intervals with `car::Confint()`")
car::Confint(davis_lm0)
```

Confidence intervals for the estimates of `davis_lm0` with `stats::confint()`
:::

***
Unlike `stats::confint()`, `car::Confint()` prints the coefficient estimates along with the confidence limits.

::::
:::::



The separate confidence intervals do not address the hypothesis that *simultaneously* $\beta_{0} = 0$ and $\beta_{1} = 1$ versus the alternative that either or both of the intercept and slope differ from these values.

:::


::::
:::::

We shoud have started with visualization of the data set! Always start with Exploratory Data Analysis (`r glossary("ExDA", "EDA")`)!

:::::{.my-example}
:::{.my-example-header}
:::::: {#exm-chap04-eda}
: Exploratory Data Analysis of Davis data set
::::::
:::
::::{.my-example-container}

::: {.panel-tabset}

###### plot(davis_lm0)

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-scatterplot-davis-lm0}
: Scatterplot of `davis_lm0` 
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-scatterplot-davis-lm0}
```{r}
#| label: scatterplot-davis-lm0

Davis <- carData::Davis

graphics::plot(weight ~ repwt, data = Davis)
graphics::abline(0, 1, lty = "dashed", lwd = 2)
graphics::abline(davis_lm0, lwd = 2)
graphics::legend(
  "bottomright",
  c("Unbiased Reporting", "Least Squares"),
  lty = c("dashed", "solid"),
  lwd = 2,
  inset = 0.02
)
base::with(Davis, car::showLabels(repwt, weight, n = 3, method = "mahal"))
```
Scatterplot of measured weight (`weight`) by reported weight (`repwt`) for Davis’s data. The solid line is the least-squares linear regression line, and the broken line is the line of unbiased reporting y = x.
:::

***

- The `graphics::plot()` function draws the basic scatterplot, to which we use `graphics::abline()` to add the line of unbiased reporting (y = x, with intercept zero and slope 1, the broken line) and the least-squares line (the solid line). 
- The `graphics::legend()` command adds a legend at the lower right of the plot, inset from the corner by 2% of the plot’s size. 
- The `car::showLabels()` function identifies the two points with the largest `r glossary("Mahalanobis")` distances from the center of the data.

::::
:::::

The resulting graph reveals an extreme outlier, Case 12. Case 21 has also unusually large values of both measured and reported weight but is in line with the rest of the data. 

It seems bizarre that an individual who weighs more than 160 kg would report her weight as less than 60 kg, but there is a simple explanation: On data entry, Subject 12’s height in centimeters and weight in kilograms were inadvertently exchanged.


###### update

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-update-davis}
: Update linar model after deleting data point 12
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-update-davis}
```{r}
#| label: update-davis
#| results: hold

glue::glue("#####################################################")
glue::glue("Updated linear model `davis_lm1`")
glue::glue("#####################################################")
(davis_lm1 <- update(davis_lm0, subset = -12))

save_data_file("chap04", davis_lm1, "davis_lm1.rds")

glue::glue("#####################################################")
glue::glue("Display summary with `car::S()`")
glue::glue("#####################################################")
glue::glue(" ")
car::S(davis_lm1)
```

Linear model of updated Davis data (`davis_lm1`)
:::

::::
:::::



###### interactive

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-davis-intractive}
: Simple linear regression of interactive Davis data
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-davis-intractive}    
```{r}
#| label: davis-interactive

Davis <- carData::Davis

gg_davis <- Davis |>
  tidyr::drop_na(repwt) |>
  tibble::rowid_to_column(var = "ID") |>
  ggplot2::ggplot(
        ggplot2::aes(
          x = repwt,
          y = weight
        )
  ) +
  ggiraph::geom_point_interactive(
    ggplot2::aes(
      x = repwt,
      y = weight,
      tooltip = ID,
      data_id = ID
    ),
    shape = 1,
    size = 2
  ) +
  ggplot2::geom_smooth(
    ggplot2::aes(
      linetype = "solid"
    ),
    formula = y ~ x,
    method = lm,
    se = FALSE,
    color = "black"
  ) +
    ggplot2::geom_smooth(
      ggplot2::aes(
        linetype = "dashed",
      ),
      formula = y ~ x,
      method = loess,
      se = FALSE,
      color = "black"
  ) +
  ggplot2::scale_linetype_discrete(
    name = "",
    label = c(
      "Least Squares",
      "Unbiased Reporting   "
    )
  ) +
  ggplot2::guides(
    linetype = ggplot2::guide_legend(position = "inside")
  ) +
  ggplot2::theme(
    legend.title = ggplot2::element_blank(),
    legend.position.inside = c(.8, .15),
    legend.box.background = ggplot2::element_rect(
                              color = "black", 
                              linewidth = 1
                              )
                 )

ggiraph::girafe(ggobj = gg_davis)

```

Interactive equivalent for linear model of `weight` regresssed on `repwt` with Davis data set
:::

***

This is my replication of the book’s Figure 4.1. There are two important qualification to make:

**1. Producing an interactive graphics with {ggiraph}** 

Instead of printing out the two points with the largest Mahalanobis distance I have applied functions from the {**ggiraph**} package to produce an interactive graphics. Click at a point in the plot to get a tooltip with the row number of this point. After you learned that the outlier is from row 12 you can delete it in the next step.

I used {**ggiraph**} for two reasons:

- I know I could use the {**ggrepel**} package (see @pak-ggrepel) to label the points but I didn't know how to calculate the two points with the largest Mahalanobis distance. There is in base R a `stats::mahalanobis()` function but I didn't succeed to provide the necessary parameter for the arguments.
- {**ggiraph**} (see @pak-ggiraph) is easy to use because it sticks not even with the {**ggplot2**} structure but alsow with their names of commands. It wasn't necessary to invest much time for learning a new package as it would be the case for learning {**plotly**}, another package for creating interactive web graphics (see @pak-plotly).

**2. New detailed specification for the {ggplot2} legend**

This is the first time, that I have moved the legend inside the graoh and framed with a box. I used the https://tidyverse.or/blog article [ggplot2 3.5.0: legends](https://www.tidyverse.org/blog/2024/02/ggplot2-3-5-0-legends/) and an article from https://r-graph-gallery.com [Building a nice legend with R and ggplot2](https://r-graph-gallery.com/239-custom-layout-legend-ggplot2.html). Helpful were also hints from a [StackOverflow post](https://stackoverflow.com/a/47669217/7322615). 



::::
:::::

###### ggrepel

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-davis-ggrepel}
: Simple linear regression	of	weight	on	repwt with {**ggplot2**} and {**ggrepel**}
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-davis-ggrepel}
```{r}
#| label: davis-ggrepel

set.seed(42)
df_temp <- carData::Davis |> 
  dplyr::select(weight, repwt) |> 
  tidyr::drop_na() 

df <- df_temp |>
  dplyr::mutate(mahal =
        {
          stats::mahalanobis(
                df_temp,
                base::colMeans(df_temp),
                stats::cov(df_temp)
                )
        }
  ) |>
  dplyr::mutate(p =
      pchisq(
        q = mahal,
        df = 1,
        lower.tail = FALSE
        )
      ) |>
  tibble::rownames_to_column(var = "ID") |>
  dplyr::mutate(ID =
        dplyr::case_when(p < .001 ~ ID,
                         p >= .001 ~ "")
  )
  
df |>
  ggplot2::ggplot(
        ggplot2::aes(
                x = repwt,
                y = weight,
                label = ID
        )
  ) +
  ggplot2::geom_point(
    shape = 1,
    size = 2
  ) +
  ggrepel::geom_text_repel() +
  ggplot2::stat_smooth(
    ggplot2::aes(linetype = "solid"),
    formula = y ~ x,
    method = lm,
    se = FALSE,
    color = "black"
  ) +
    ggplot2::geom_smooth(
      ggplot2::aes(
          linetype = "dashed"
        ),
      formula = y ~ x,
      method = loess,
      se = FALSE,
      color = "black"
  ) +
  ggplot2::scale_linetype_discrete(
    name = "",
    label = c(
      "Least Squares",
      "Unbiased Reporting   "
    )
  ) +
  ggplot2::guides(
    linetype = ggplot2::guide_legend(position = "inside")
  ) +
  ggplot2::theme(
    legend.title = ggplot2::element_blank(),
    legend.position.inside = c(.8, .15),
    legend.box.background = ggplot2::element_rect(
                              color = "black",
                              linewidth = 1
                              )
                 )

```

Listing title
:::

::::
:::::

###### Anscombe

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-ID-text}
: Numbered R Code Title
::::::
:::
::::{.my-r-code-container}
::: {#lst-chaplisting-ID}
```{r}
#| label: anscombe

datasets::anscombe |> 
  ggplot2::ggplot(
    ggplot2::aes(
      x = x3,
      y = y3,
      label = rownames(anscombe)
    )
  ) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(
    formula = y ~ x,
    se = FALSE,
    method = lm
  ) +
  ggrepel::geom_text_repel()
```

Listing title
:::

::::
:::::




###### interactive2

:::::{.my-r-code}
:::{.my-r-code-header}
:::::: {#cnj-chap04-davis-intractive2}
: Simple linear regression of updated interactive Davis data
::::::
:::
::::{.my-r-code-container}
::: {#lst-chap04-davis-intractive2}    
```{r}
#| label: davis-interactive2

gg_davis <- Davis |>
  tidyr::drop_na(repwt) |>
  tibble::rowid_to_column(var = "ID") |>
  dplyr::filter(ID != 12) |> 
  ggplot2::ggplot(
        ggplot2::aes(
          x = repwt,
          y = weight
        )
  ) +
  ggiraph::geom_point_interactive(
    ggplot2::aes(
      x = repwt,
      y = weight,
      tooltip = ID,
      data_id = ID
    ),
    shape = 1,
    size = 2
  ) +
  ggplot2::geom_smooth(
    formula = y ~ x,
    method = lm,
    se = FALSE,
    color = "black"
  ) +
    ggplot2::geom_smooth(
    formula = y ~ x,
    method = loess,
    se = FALSE,
    linetype = "dashed",
    color = "black"
  )

ggiraph::girafe(ggobj = gg_davis)

```

Interactive equivalent for updated linear model of `weight` regresssed on `repwt` with Davis data set
:::

***


::::
:::::

:::

::::
:::::

***
